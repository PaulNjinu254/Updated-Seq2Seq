{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1zvnSALrizVMFtQAZfpk2soaqkmyLljys",
      "authorship_tag": "ABX9TyNWTKLIT9AaoaiHejrYasUT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulNjinu254/Updated-Seq2Seq/blob/main/Copy_of_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Title: Character-level recurrent sequence-to-sequence model\n",
        "Author: [fchollet](https://twitter.com/fchollet)\n",
        "Date created: 2017/09/29\n",
        "Last modified: 2023/11/22\n",
        "Description: Character-level recurrent sequence-to-sequence model.\n",
        "Accelerator: GPU\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "## Introduction\n",
        "\n",
        "This example demonstrates how to implement a basic character-level\n",
        "recurrent sequence-to-sequence model. We apply it to translating\n",
        "short English sentences into short French sentences,\n",
        "character-by-character. Note that it is fairly unusual to\n",
        "do character-level machine translation, as word-level\n",
        "models are more common in this domain.\n",
        "\n",
        "**Summary of the algorithm**\n",
        "\n",
        "- We start with input sequences from a domain (e.g. English sentences)\n",
        "    and corresponding target sequences from another domain\n",
        "    (e.g. French sentences).\n",
        "- An encoder LSTM turns input sequences to 2 state vectors\n",
        "    (we keep the last LSTM state and discard the outputs).\n",
        "- A decoder LSTM is trained to turn the target sequences into\n",
        "    the same sequence but offset by one timestep in the future,\n",
        "    a training process called \"teacher forcing\" in this context.\n",
        "    It uses as initial state the state vectors from the encoder.\n",
        "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
        "    given `targets[...t]`, conditioned on the input sequence.\n",
        "- In inference mode, when we want to decode unknown input sequences, we:\n",
        "    - Encode the input sequence into state vectors\n",
        "    - Start with a target sequence of size 1\n",
        "        (just the start-of-sequence character)\n",
        "    - Feed the state vectors and 1-char target sequence\n",
        "        to the decoder to produce predictions for the next character\n",
        "    - Sample the next character using these predictions\n",
        "        (we simply use argmax).\n",
        "    - Append the sampled character to the target sequence\n",
        "    - Repeat until we generate the end-of-sequence character or we\n",
        "        hit the character limit.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "## Setup\n",
        "\"\"\"\n",
        "\n",
        "# ==============================\n",
        "# Imports\n",
        "# ==============================\n",
        "import numpy as np\n",
        "import keras\n",
        "import os\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# ==============================\n",
        "# Download and extract dataset with User-Agent spoof\n",
        "# ==============================\n",
        "url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                  \"Chrome/115.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "print(\"Downloading dataset...\")\n",
        "r = requests.get(url, headers=headers)\n",
        "r.raise_for_status()\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
        "    z.extractall(\"data\")\n",
        "\n",
        "print(\"Dataset downloaded and extracted.\")\n",
        "\n",
        "data_path = os.path.join(\"data\", \"fra.txt\")\n",
        "\n",
        "# ==============================\n",
        "# Configuration\n",
        "# ==============================\n",
        "batch_size = 64      # Batch size for training\n",
        "epochs = 100         # Number of epochs to train for\n",
        "latent_dim = 256     # Latent dimensionality of the encoding space\n",
        "num_samples = 10000  # Number of samples to train on\n",
        "\n",
        "# ==============================\n",
        "# Prepare the data\n",
        "# ==============================\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"  # start & end tokens\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    input_characters.update(list(input_text))\n",
        "    target_characters.update(list(target_text))\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max(len(txt) for txt in input_texts)\n",
        "max_decoder_seq_length = max(len(txt) for txt in target_texts)\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "input_token_index = {char: i for i, char in enumerate(input_characters)}\n",
        "target_token_index = {char: i for i, char in enumerate(target_characters)}\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "# ==============================\n",
        "# Build the model\n",
        "# ==============================\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# ==============================\n",
        "# Train the model\n",
        "# ==============================\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "model.save(\"s2s_model.keras\")\n",
        "\n",
        "# ==============================\n",
        "# Inference models\n",
        "# ==============================\n",
        "model = keras.models.load_model(\"s2s_model.keras\")\n",
        "\n",
        "encoder_inputs = model.input[0]\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output\n",
        "encoder_model = keras.Model(encoder_inputs, [state_h_enc, state_c_enc])\n",
        "\n",
        "decoder_inputs = model.input[1]\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "reverse_input_char_index = {i: char for char, i in input_token_index.items()}\n",
        "reverse_target_char_index = {i: char for char, i in target_token_index.items()}\n",
        "\n",
        "# ==============================\n",
        "# Decode function\n",
        "# ==============================\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    decoded_sentence = \"\"\n",
        "    while True:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            break\n",
        "\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "# ==============================\n",
        "# Test decoding\n",
        "# ==============================\n",
        "for seq_index in range(20):\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", input_texts[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtZBYGNI29uw",
        "outputId": "2b3e73f4-dc49-4457-f3c1-e27c8ff2eaf2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Dataset downloaded and extracted.\n",
            "Number of samples: 10000\n",
            "Number of unique input tokens: 70\n",
            "Number of unique output tokens: 91\n",
            "Max sequence length for inputs: 14\n",
            "Max sequence length for outputs: 59\n",
            "Epoch 1/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 348ms/step - accuracy: 0.7053 - loss: 1.5669 - val_accuracy: 0.7175 - val_loss: 1.0727\n",
            "Epoch 2/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 357ms/step - accuracy: 0.7463 - loss: 0.9729 - val_accuracy: 0.7183 - val_loss: 0.9866\n",
            "Epoch 3/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 370ms/step - accuracy: 0.7629 - loss: 0.8627 - val_accuracy: 0.7502 - val_loss: 0.8693\n",
            "Epoch 4/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 346ms/step - accuracy: 0.7868 - loss: 0.7744 - val_accuracy: 0.7775 - val_loss: 0.7724\n",
            "Epoch 5/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 355ms/step - accuracy: 0.8020 - loss: 0.6963 - val_accuracy: 0.7872 - val_loss: 0.7200\n",
            "Epoch 6/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 352ms/step - accuracy: 0.8146 - loss: 0.6408 - val_accuracy: 0.7950 - val_loss: 0.6949\n",
            "Epoch 7/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 363ms/step - accuracy: 0.8228 - loss: 0.6079 - val_accuracy: 0.8056 - val_loss: 0.6631\n",
            "Epoch 8/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 358ms/step - accuracy: 0.8290 - loss: 0.5858 - val_accuracy: 0.8113 - val_loss: 0.6436\n",
            "Epoch 9/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 343ms/step - accuracy: 0.8354 - loss: 0.5614 - val_accuracy: 0.8163 - val_loss: 0.6245\n",
            "Epoch 10/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 364ms/step - accuracy: 0.8406 - loss: 0.5442 - val_accuracy: 0.8215 - val_loss: 0.6099\n",
            "Epoch 11/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 343ms/step - accuracy: 0.8469 - loss: 0.5249 - val_accuracy: 0.8262 - val_loss: 0.5928\n",
            "Epoch 12/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 342ms/step - accuracy: 0.8513 - loss: 0.5096 - val_accuracy: 0.8325 - val_loss: 0.5780\n",
            "Epoch 13/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 364ms/step - accuracy: 0.8553 - loss: 0.4943 - val_accuracy: 0.8355 - val_loss: 0.5696\n",
            "Epoch 14/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 352ms/step - accuracy: 0.8585 - loss: 0.4858 - val_accuracy: 0.8390 - val_loss: 0.5572\n",
            "Epoch 15/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 355ms/step - accuracy: 0.8619 - loss: 0.4706 - val_accuracy: 0.8385 - val_loss: 0.5534\n",
            "Epoch 16/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 354ms/step - accuracy: 0.8649 - loss: 0.4590 - val_accuracy: 0.8389 - val_loss: 0.5482\n",
            "Epoch 17/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 351ms/step - accuracy: 0.8658 - loss: 0.4548 - val_accuracy: 0.8448 - val_loss: 0.5322\n",
            "Epoch 18/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 374ms/step - accuracy: 0.8669 - loss: 0.4485 - val_accuracy: 0.8428 - val_loss: 0.5321\n",
            "Epoch 19/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 363ms/step - accuracy: 0.8701 - loss: 0.4390 - val_accuracy: 0.8474 - val_loss: 0.5183\n",
            "Epoch 20/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 363ms/step - accuracy: 0.8718 - loss: 0.4312 - val_accuracy: 0.8499 - val_loss: 0.5109\n",
            "Epoch 21/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 370ms/step - accuracy: 0.8755 - loss: 0.4184 - val_accuracy: 0.8496 - val_loss: 0.5106\n",
            "Epoch 22/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 358ms/step - accuracy: 0.8772 - loss: 0.4133 - val_accuracy: 0.8524 - val_loss: 0.5033\n",
            "Epoch 23/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 344ms/step - accuracy: 0.8781 - loss: 0.4084 - val_accuracy: 0.8540 - val_loss: 0.5002\n",
            "Epoch 24/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 362ms/step - accuracy: 0.8817 - loss: 0.3977 - val_accuracy: 0.8546 - val_loss: 0.4951\n",
            "Epoch 25/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 350ms/step - accuracy: 0.8816 - loss: 0.3976 - val_accuracy: 0.8553 - val_loss: 0.4910\n",
            "Epoch 26/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 352ms/step - accuracy: 0.8839 - loss: 0.3903 - val_accuracy: 0.8558 - val_loss: 0.4877\n",
            "Epoch 27/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 354ms/step - accuracy: 0.8856 - loss: 0.3840 - val_accuracy: 0.8593 - val_loss: 0.4815\n",
            "Epoch 28/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 361ms/step - accuracy: 0.8882 - loss: 0.3769 - val_accuracy: 0.8602 - val_loss: 0.4795\n",
            "Epoch 29/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 358ms/step - accuracy: 0.8903 - loss: 0.3689 - val_accuracy: 0.8597 - val_loss: 0.4778\n",
            "Epoch 30/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 358ms/step - accuracy: 0.8900 - loss: 0.3670 - val_accuracy: 0.8607 - val_loss: 0.4726\n",
            "Epoch 31/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 354ms/step - accuracy: 0.8923 - loss: 0.3603 - val_accuracy: 0.8612 - val_loss: 0.4742\n",
            "Epoch 32/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 365ms/step - accuracy: 0.8951 - loss: 0.3526 - val_accuracy: 0.8627 - val_loss: 0.4687\n",
            "Epoch 33/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 358ms/step - accuracy: 0.8966 - loss: 0.3470 - val_accuracy: 0.8635 - val_loss: 0.4652\n",
            "Epoch 34/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 337ms/step - accuracy: 0.8971 - loss: 0.3445 - val_accuracy: 0.8637 - val_loss: 0.4666\n",
            "Epoch 35/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 343ms/step - accuracy: 0.8978 - loss: 0.3411 - val_accuracy: 0.8655 - val_loss: 0.4617\n",
            "Epoch 36/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 336ms/step - accuracy: 0.9009 - loss: 0.3326 - val_accuracy: 0.8663 - val_loss: 0.4587\n",
            "Epoch 37/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 340ms/step - accuracy: 0.9018 - loss: 0.3280 - val_accuracy: 0.8664 - val_loss: 0.4597\n",
            "Epoch 38/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 363ms/step - accuracy: 0.9035 - loss: 0.3235 - val_accuracy: 0.8671 - val_loss: 0.4561\n",
            "Epoch 39/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 345ms/step - accuracy: 0.9044 - loss: 0.3188 - val_accuracy: 0.8669 - val_loss: 0.4577\n",
            "Epoch 40/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 338ms/step - accuracy: 0.9048 - loss: 0.3194 - val_accuracy: 0.8676 - val_loss: 0.4561\n",
            "Epoch 41/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 350ms/step - accuracy: 0.9067 - loss: 0.3127 - val_accuracy: 0.8683 - val_loss: 0.4506\n",
            "Epoch 42/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 366ms/step - accuracy: 0.9075 - loss: 0.3087 - val_accuracy: 0.8671 - val_loss: 0.4566\n",
            "Epoch 43/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 357ms/step - accuracy: 0.9087 - loss: 0.3063 - val_accuracy: 0.8688 - val_loss: 0.4510\n",
            "Epoch 44/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 357ms/step - accuracy: 0.9094 - loss: 0.3021 - val_accuracy: 0.8692 - val_loss: 0.4508\n",
            "Epoch 45/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 361ms/step - accuracy: 0.9111 - loss: 0.2965 - val_accuracy: 0.8698 - val_loss: 0.4499\n",
            "Epoch 46/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 357ms/step - accuracy: 0.9127 - loss: 0.2910 - val_accuracy: 0.8696 - val_loss: 0.4495\n",
            "Epoch 47/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 366ms/step - accuracy: 0.9134 - loss: 0.2883 - val_accuracy: 0.8711 - val_loss: 0.4470\n",
            "Epoch 48/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 351ms/step - accuracy: 0.9138 - loss: 0.2855 - val_accuracy: 0.8699 - val_loss: 0.4504\n",
            "Epoch 49/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 365ms/step - accuracy: 0.9156 - loss: 0.2820 - val_accuracy: 0.8718 - val_loss: 0.4482\n",
            "Epoch 50/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 336ms/step - accuracy: 0.9161 - loss: 0.2796 - val_accuracy: 0.8721 - val_loss: 0.4478\n",
            "Epoch 51/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 340ms/step - accuracy: 0.9174 - loss: 0.2745 - val_accuracy: 0.8718 - val_loss: 0.4498\n",
            "Epoch 52/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 339ms/step - accuracy: 0.9182 - loss: 0.2708 - val_accuracy: 0.8726 - val_loss: 0.4462\n",
            "Epoch 53/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 367ms/step - accuracy: 0.9201 - loss: 0.2653 - val_accuracy: 0.8721 - val_loss: 0.4513\n",
            "Epoch 54/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 374ms/step - accuracy: 0.9204 - loss: 0.2640 - val_accuracy: 0.8720 - val_loss: 0.4472\n",
            "Epoch 55/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 369ms/step - accuracy: 0.9218 - loss: 0.2614 - val_accuracy: 0.8729 - val_loss: 0.4498\n",
            "Epoch 56/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 355ms/step - accuracy: 0.9225 - loss: 0.2583 - val_accuracy: 0.8718 - val_loss: 0.4518\n",
            "Epoch 57/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 367ms/step - accuracy: 0.9241 - loss: 0.2527 - val_accuracy: 0.8734 - val_loss: 0.4470\n",
            "Epoch 58/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 343ms/step - accuracy: 0.9252 - loss: 0.2486 - val_accuracy: 0.8740 - val_loss: 0.4489\n",
            "Epoch 59/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 364ms/step - accuracy: 0.9251 - loss: 0.2480 - val_accuracy: 0.8725 - val_loss: 0.4519\n",
            "Epoch 60/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 370ms/step - accuracy: 0.9262 - loss: 0.2450 - val_accuracy: 0.8738 - val_loss: 0.4514\n",
            "Epoch 61/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 359ms/step - accuracy: 0.9276 - loss: 0.2408 - val_accuracy: 0.8742 - val_loss: 0.4479\n",
            "Epoch 62/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 363ms/step - accuracy: 0.9287 - loss: 0.2382 - val_accuracy: 0.8753 - val_loss: 0.4498\n",
            "Epoch 63/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 355ms/step - accuracy: 0.9295 - loss: 0.2355 - val_accuracy: 0.8739 - val_loss: 0.4542\n",
            "Epoch 64/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 342ms/step - accuracy: 0.9307 - loss: 0.2298 - val_accuracy: 0.8740 - val_loss: 0.4547\n",
            "Epoch 65/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 378ms/step - accuracy: 0.9317 - loss: 0.2287 - val_accuracy: 0.8744 - val_loss: 0.4546\n",
            "Epoch 66/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 334ms/step - accuracy: 0.9331 - loss: 0.2234 - val_accuracy: 0.8736 - val_loss: 0.4591\n",
            "Epoch 67/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 340ms/step - accuracy: 0.9331 - loss: 0.2229 - val_accuracy: 0.8746 - val_loss: 0.4590\n",
            "Epoch 68/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 352ms/step - accuracy: 0.9340 - loss: 0.2193 - val_accuracy: 0.8741 - val_loss: 0.4630\n",
            "Epoch 69/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 371ms/step - accuracy: 0.9348 - loss: 0.2165 - val_accuracy: 0.8746 - val_loss: 0.4605\n",
            "Epoch 70/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 344ms/step - accuracy: 0.9354 - loss: 0.2156 - val_accuracy: 0.8750 - val_loss: 0.4656\n",
            "Epoch 71/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 356ms/step - accuracy: 0.9368 - loss: 0.2105 - val_accuracy: 0.8747 - val_loss: 0.4634\n",
            "Epoch 72/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 347ms/step - accuracy: 0.9374 - loss: 0.2089 - val_accuracy: 0.8751 - val_loss: 0.4667\n",
            "Epoch 73/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 342ms/step - accuracy: 0.9387 - loss: 0.2047 - val_accuracy: 0.8749 - val_loss: 0.4687\n",
            "Epoch 74/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 352ms/step - accuracy: 0.9401 - loss: 0.2005 - val_accuracy: 0.8748 - val_loss: 0.4675\n",
            "Epoch 75/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 349ms/step - accuracy: 0.9396 - loss: 0.2010 - val_accuracy: 0.8745 - val_loss: 0.4692\n",
            "Epoch 76/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 342ms/step - accuracy: 0.9412 - loss: 0.1965 - val_accuracy: 0.8746 - val_loss: 0.4724\n",
            "Epoch 77/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 354ms/step - accuracy: 0.9417 - loss: 0.1931 - val_accuracy: 0.8746 - val_loss: 0.4728\n",
            "Epoch 78/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 338ms/step - accuracy: 0.9416 - loss: 0.1930 - val_accuracy: 0.8747 - val_loss: 0.4787\n",
            "Epoch 79/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 368ms/step - accuracy: 0.9428 - loss: 0.1904 - val_accuracy: 0.8739 - val_loss: 0.4782\n",
            "Epoch 80/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 339ms/step - accuracy: 0.9437 - loss: 0.1874 - val_accuracy: 0.8752 - val_loss: 0.4794\n",
            "Epoch 81/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 342ms/step - accuracy: 0.9443 - loss: 0.1856 - val_accuracy: 0.8743 - val_loss: 0.4838\n",
            "Epoch 82/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 357ms/step - accuracy: 0.9452 - loss: 0.1825 - val_accuracy: 0.8741 - val_loss: 0.4859\n",
            "Epoch 83/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 355ms/step - accuracy: 0.9451 - loss: 0.1806 - val_accuracy: 0.8740 - val_loss: 0.4830\n",
            "Epoch 84/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 357ms/step - accuracy: 0.9474 - loss: 0.1755 - val_accuracy: 0.8736 - val_loss: 0.4890\n",
            "Epoch 85/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 351ms/step - accuracy: 0.9471 - loss: 0.1754 - val_accuracy: 0.8748 - val_loss: 0.4894\n",
            "Epoch 86/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 370ms/step - accuracy: 0.9475 - loss: 0.1734 - val_accuracy: 0.8745 - val_loss: 0.4903\n",
            "Epoch 87/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 349ms/step - accuracy: 0.9484 - loss: 0.1720 - val_accuracy: 0.8743 - val_loss: 0.4937\n",
            "Epoch 88/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 345ms/step - accuracy: 0.9496 - loss: 0.1678 - val_accuracy: 0.8731 - val_loss: 0.5011\n",
            "Epoch 89/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 347ms/step - accuracy: 0.9500 - loss: 0.1658 - val_accuracy: 0.8740 - val_loss: 0.4967\n",
            "Epoch 90/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 336ms/step - accuracy: 0.9510 - loss: 0.1617 - val_accuracy: 0.8744 - val_loss: 0.5019\n",
            "Epoch 91/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 368ms/step - accuracy: 0.9518 - loss: 0.1610 - val_accuracy: 0.8747 - val_loss: 0.5046\n",
            "Epoch 92/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 343ms/step - accuracy: 0.9527 - loss: 0.1584 - val_accuracy: 0.8733 - val_loss: 0.5077\n",
            "Epoch 93/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 336ms/step - accuracy: 0.9520 - loss: 0.1580 - val_accuracy: 0.8750 - val_loss: 0.5060\n",
            "Epoch 94/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 347ms/step - accuracy: 0.9531 - loss: 0.1548 - val_accuracy: 0.8731 - val_loss: 0.5103\n",
            "Epoch 95/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 364ms/step - accuracy: 0.9541 - loss: 0.1529 - val_accuracy: 0.8743 - val_loss: 0.5141\n",
            "Epoch 96/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 353ms/step - accuracy: 0.9542 - loss: 0.1506 - val_accuracy: 0.8736 - val_loss: 0.5170\n",
            "Epoch 97/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 350ms/step - accuracy: 0.9544 - loss: 0.1511 - val_accuracy: 0.8732 - val_loss: 0.5179\n",
            "Epoch 98/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 337ms/step - accuracy: 0.9552 - loss: 0.1479 - val_accuracy: 0.8737 - val_loss: 0.5181\n",
            "Epoch 99/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 345ms/step - accuracy: 0.9561 - loss: 0.1458 - val_accuracy: 0.8735 - val_loss: 0.5234\n",
            "Epoch 100/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 361ms/step - accuracy: 0.9564 - loss: 0.1440 - val_accuracy: 0.8742 - val_loss: 0.5236\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Cours !\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Cours !\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Cours !\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Cours !\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Salut !\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Salut !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Fuyez !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Fuyez !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Fuyez !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Fuyez !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Fuyez !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Fuyez !\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import zipfile\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Google Drive mount\n",
        "from google.colab import drive\n",
        "print(\"[INFO] Mounting Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# Paths\n",
        "MODEL_ZIP_PATH = \"/content/drive/MyDrive/pretrained_model.zip\"\n",
        "EXTRACT_DIR = \"/content/pretrained_model\"\n",
        "VOCAB_PATH = \"/content/drive/MyDrive/vocab.pkl\"\n",
        "IMAGES_DIR = \"/content/drive/MyDrive/Graduation_Photos\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/captions_results.txt\"\n",
        "\n",
        "# Vocabulary class\n",
        "class Vocabulary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        return self.word2idx.get(word, self.word2idx.get('<unk>', 0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "    def idx2word(self, idx):\n",
        "        return self.idx2word.get(idx, '<unk>')\n",
        "\n",
        "# Extract model zip\n",
        "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "if MODEL_ZIP_PATH and os.path.exists(MODEL_ZIP_PATH):\n",
        "    print(f\"[INFO] Extracting model zip {MODEL_ZIP_PATH} -> {EXTRACT_DIR} ...\")\n",
        "    with zipfile.ZipFile(MODEL_ZIP_PATH, 'r') as z:\n",
        "        z.extractall(EXTRACT_DIR)\n",
        "else:\n",
        "    print(f\"[INFO] Model zip not found at {MODEL_ZIP_PATH}. If your encoder/decoder are already extracted, ensure they are in {EXTRACT_DIR}.\")\n",
        "\n",
        "print(\"[INFO] Extract dir contents:\", os.listdir(EXTRACT_DIR))\n",
        "\n",
        "# Load vocabulary (robust to different pickle formats)\n",
        "print(\"[INFO] Loading vocabulary from\", VOCAB_PATH)\n",
        "with open(VOCAB_PATH, 'rb') as f:\n",
        "    raw_vocab = pickle.load(f)\n",
        "\n",
        "# Normalize vocab into an object with word2idx and idx2word attributes\n",
        "if isinstance(raw_vocab, Vocabulary):\n",
        "    vocab_obj = raw_vocab\n",
        "elif isinstance(raw_vocab, dict):\n",
        "    # try to detect common formats\n",
        "    if 'word2idx' in raw_vocab and 'idx2word' in raw_vocab:\n",
        "        vocab_obj = Vocabulary()\n",
        "        vocab_obj.word2idx = raw_vocab['word2idx']\n",
        "        vocab_obj.idx2word = raw_vocab['idx2word']\n",
        "    elif 'stoi' in raw_vocab and 'itos' in raw_vocab:\n",
        "        vocab_obj = Vocabulary()\n",
        "        vocab_obj.word2idx = raw_vocab['stoi']\n",
        "        # build idx2word\n",
        "        vocab_obj.idx2word = {i: w for i, w in enumerate(raw_vocab['itos'])}\n",
        "    elif 'itos' in raw_vocab:\n",
        "        # itos = list index->word\n",
        "        itos = raw_vocab['itos']\n",
        "        vocab_obj = Vocabulary()\n",
        "        vocab_obj.idx2word = {i: w for i, w in enumerate(itos)}\n",
        "        vocab_obj.word2idx = {w: i for i, w in enumerate(itos)}\n",
        "    else:\n",
        "        try:\n",
        "            # detect if keys are ints\n",
        "            if all(isinstance(k, int) for k in raw_vocab.keys()):\n",
        "                vocab_obj = Vocabulary()\n",
        "                vocab_obj.idx2word = {int(k): v for k, v in raw_vocab.items()}\n",
        "                vocab_obj.word2idx = {v: int(k) for k, v in raw_vocab.items()}\n",
        "            else:\n",
        "                # assume it's word->idx\n",
        "                vocab_obj = Vocabulary()\n",
        "                vocab_obj.word2idx = {k: int(v) for k, v in raw_vocab.items()}\n",
        "                vocab_obj.idx2word = {int(v): k for k, v in raw_vocab.items()}\n",
        "        except Exception:\n",
        "            raise RuntimeError(\"Unrecognized vocab.pkl format. Inspect the pickle content.\")\n",
        "else:\n",
        "    raise RuntimeError(\"Unrecognized vocab.pkl type. Expected Vocabulary object or dict.\")\n",
        "\n",
        "print(f\"[INFO] Vocab loaded. Size = {len(vocab_obj)} words\")\n",
        "\n",
        "# Helper for idx->word\n",
        "def idx_to_word(idx):\n",
        "    # vocab_obj stores idx2word mapping as dict\n",
        "    return vocab_obj.idx2word.get(int(idx), '<unk>')\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"[INFO] Device:\", device)\n",
        "\n",
        "# Model definitions\n",
        "import torchvision.models as models\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size=256):\n",
        "        super().__init__()\n",
        "        # using resnet152 feature extractor\n",
        "        resnet = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V2)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "\n",
        "    def forward(self, images):\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "        return features\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.embed(captions[:, :-1])\n",
        "        inputs = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
        "        hiddens, _ = self.lstm(inputs)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n",
        "\n",
        "    def sample(self, features, states=None, max_len=20):\n",
        "        sampled_ids =_\n",
        "\n",
        "\n",
        "    def sample(self, features, states=None, max_len=20):\n",
        "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
        "        sampled_ids = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "        for _ in range(max_len):\n",
        "            hiddens, states = self.lstm(inputs, states)\n",
        "            outputs = self.linear(hiddens.squeeze(1))\n",
        "            _, predicted = outputs.max(1)\n",
        "            sampled_ids.append(predicted.item())\n",
        "            if idx_to_word(predicted.item()) == '<end>':\n",
        "                break\n",
        "            inputs = self.embed(predicted).unsqueeze(1)\n",
        "        return sampled_ids\n",
        "\n",
        "# 7) Load trained model weights\n",
        "print(\"[INFO] Loading pretrained models...\")\n",
        "encoder_path = os.path.join(EXTRACT_DIR, \"encoder-5-3000.pkl\")\n",
        "decoder_path = os.path.join(EXTRACT_DIR, \"decoder-5-3000.pkl\")\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "vocab_size = len(vocab_obj)\n",
        "\n",
        "encoder = EncoderCNN(embed_size).to(device)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
        "\n",
        "encoder.load_state_dict(torch.load(encoder_path, map_location=device))\n",
        "decoder.load_state_dict(torch.load(decoder_path, map_location=device))\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "print(\"[INFO] Models loaded successfully.\")\n",
        "\n",
        "# 8) Image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                         std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Caption generation function\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = encoder(image_tensor)\n",
        "        sampled_ids = decoder.sample(features)\n",
        "    words = [idx_to_word(word_id) for word_id in sampled_ids]\n",
        "    # remove <start> and <end> tokens\n",
        "    words = [w for w in words if w not in [\"<start>\", \"<end>\"]]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Run captioning on images\n",
        "results = []\n",
        "image_files = [f for f in os.listdir(IMAGES_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "for img_name in image_files:\n",
        "    img_path = os.path.join(IMAGES_DIR, img_name)\n",
        "    caption = generate_caption(img_path)\n",
        "    print(f\"{img_name} -> {caption}\")\n",
        "    results.append(f\"{img_name}\\t{caption}\")\n",
        "\n",
        "# Save results\n",
        "with open(OUTPUT_PATH, \"w\") as f:\n",
        "    for line in results:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "print(f\"[INFO] Captions saved to {OUTPUT_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX9xfF_KbQvZ",
        "outputId": "33161f81-aeb6-4907-8eac-73c7d4cd609b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[INFO] Extracting model zip /content/drive/MyDrive/pretrained_model.zip -> /content/pretrained_model ...\n",
            "[INFO] Extract dir contents: ['encoder-5-3000.pkl', 'decoder-5-3000.pkl']\n",
            "[INFO] Loading vocabulary from /content/drive/MyDrive/vocab.pkl\n",
            "[INFO] Vocab loaded. Size = 9956 words\n",
            "[INFO] Device: cpu\n",
            "[INFO] Loading pretrained models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-f82ba261.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-f82ba261.pth\n",
            "100%|██████████| 230M/230M [00:03<00:00, 78.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Models loaded successfully.\n",
            "IMG_20240313_173320.jpg -> a bunch of different types of scissors on a table .\n",
            "IMG_9837.JPG -> a person holding a frisbee in his mouth .\n",
            "njinu 1.jpg -> a man wearing a hat and tie with a hat .\n",
            "IMG-20231123-WA0010.jpg -> a man with a hat and a tie in his hand .\n",
            "IMG20231124092344.jpg -> a man wearing a suit and tie with a hat .\n",
            "DSC_2026.JPG -> a man holding a baby in a blanket on a bed .\n",
            "IMG_20231124_085132.jpg -> a man and woman in a green dress holding a frisbee .\n",
            "IMG-20231123-WA0015.jpg -> a man with a hat and a tie in his hand .\n",
            "IMG-20231123-WA0002.jpg -> a group of people standing in front of a large crowd .\n",
            "[INFO] Captions saved to /content/drive/MyDrive/captions_results.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running It with Keras Instead of PyTorch\n",
        "'''\n",
        "If you have a PyTorch implementation but want to run it in Keras:\n",
        "\n",
        "Steps:\n",
        "\n",
        "Model Architecture Conversion\n",
        "\n",
        "Identify the layers and structure in the PyTorch model.py.\n",
        "\n",
        "Recreate the architecture in Keras using tf.keras.layers equivalents (e.g., nn.Linear → Dense, nn.Conv2d → Conv2D, nn.LSTM → LSTM).\n",
        "\n",
        "Keep parameter sizes identical so weights can be mapped.\n",
        "\n",
        "Weight Conversion\n",
        "\n",
        "PyTorch and Keras use different formats for weights.\n",
        "\n",
        "Use a library like onnx to export the PyTorch model to the ONNX format, then load into TensorFlow/Keras via onnx-tf or tf2onnx.\n",
        "\n",
        "Alternatively, manually load the .pth file, extract tensors with state_dict(), and assign them to Keras layers with layer.set_weights(), making sure dimensions match.\n",
        "\n",
        "Tokenizer & Data Preprocessing\n",
        "\n",
        "Replace PyTorch text/image preprocessing (torchvision.transforms, custom tokenizers) with Keras equivalents (tf.image, Tokenizer, TextVectorization).\n",
        "\n",
        "Training / Inference Adjustments\n",
        "\n",
        "Inference steps (model.eval() in PyTorch) translate to model.predict() in Keras.\n",
        "\n",
        "Batch handling will be via tf.data pipelines instead of PyTorch DataLoader.\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "Translating Between Japanese and English\n",
        "Steps:\n",
        "\n",
        "Use a Japanese tokenizer like MeCab or SentencePiece (because Japanese text does not have spaces).\n",
        "\n",
        "Prepare parallel corpus (e.g., JESC, Tatoeba, or Kyoto Free Translation Task dataset).\n",
        "\n",
        "Train a Seq2Seq or Transformer model (Hugging Face’s MarianMT or T5 works well).\n",
        "\n",
        "For inference, ensure proper preprocessing:\n",
        "\n",
        "Japanese → tokenization (subwords)\n",
        "\n",
        "English → detokenization\n",
        "\n",
        "\n",
        "\n",
        "Advanced Machine Translation Methods\n",
        "\n",
        "Attention Mechanisms (Bahdanau, Luong)\n",
        "\n",
        "Transformers (Vaswani et al., 2017) — models like BERT, GPT, and MarianMT.\n",
        "\n",
        "Multilingual Models — single model trained on multiple languages (mBART, mT5).\n",
        "\n",
        "Pre-trained Models with Fine-tuning — start with a general MT model, fine-tune on your specific domain.\n",
        "\n",
        "\n",
        "\n",
        "Generating Images from Text (Opposite of Captioning)\n",
        "Techniques:\n",
        "\n",
        "Diffusion Models (e.g., Stable Diffusion, DALL·E 2, MidJourney)\n",
        "\n",
        "GANs (StackGAN, AttnGAN — text-conditioned image generation)\n",
        "\n",
        "CLIP + Diffusion (guiding generation with text embeddings)\n",
        "\n",
        "Neural Rendering (NeRF-based, though mainly 3D)\n",
        "\n",
        "Basic Flow for Text-to-Image:\n",
        "\n",
        "Encode text into vector representation (BERT/CLIP encoder).\n",
        "\n",
        "Feed into generative model (Diffusion or GAN).\n",
        "\n",
        "Generate image pixels conditioned on the text embedding.\n",
        "'''"
      ],
      "metadata": {
        "id": "fNkb11_WPYkO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}